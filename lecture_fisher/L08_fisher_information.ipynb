{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L08: Fisher Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources and additional reading:**\n",
    "- Jaynes: Probability Theory, Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion of the posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap from the previous lecture that we can Taylor expand the log-posterior around the MAP to second order as:\n",
    "\n",
    "$$\\Delta \\chi^2(\\boldsymbol{\\theta})=({\\theta}_\\alpha-\\hat{{\\theta}}_{\\mathrm{MAP}\\alpha})^\\mathrm{T}\\frac{1}{2}\\frac{\\partial^2\\chi^2}{\\partial {\\theta}_\\alpha\\partial {\\theta}_\\beta}({\\theta}_\\beta-\\hat{\\theta}_{{\\mathrm{MAP}\\beta}})=\\chi^2(\\boldsymbol{\\theta})-\\chi^2_{\\mathrm{min}}\\,,$$\n",
    "\n",
    "where we assumed the sum convention and greek indices will always run over all parameters $\\alpha = 1,\\dots,n$. The resulting posterior has the form of an $n$-dimensional Gaussian with probability density:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}|\\boldsymbol{D}) = \\frac{\\sqrt{\\mathrm{det}{\\boldsymbol{\\mathrm{F}}}}}{\\sqrt{(2\\pi)^n}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})^\\mathrm{T}\\, \\boldsymbol{\\mathrm{F}} \\, (\\boldsymbol{\\theta}-\\hat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}})\\right)\\,,$$\n",
    "\n",
    "Where we can identify the matrix $\\boldsymbol{\\mathrm{F}}$ as the inverse of the covariance matrix, $\\boldsymbol{\\mathrm{C}}^{-1}$, of the parameters. Note that the posterior still explicitly depends on the data via the MAP. With this, the inference problem would boil down to finding $\\hat{\\boldsymbol{\\theta}}_{\\mathrm{MAP}}$ and than calculating the confidence regions directly from the previous expression as discussed in L07.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fisher matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we are faced with the situation where we do not have the data yet. Experiments in physics nowadays cost billions of Euros. Trial and error is therefore not an option. However, if the data is not measured yet, all possible outcomes must be considered. Hence we should average over all data, a procedure known as forecasting, since we would like to know the posterior distribution of a measurement before carrying it out. The only quantities which depend on the data is $\\hat{\\boldsymbol{\\theta}}_\\mathrm{MAP}$ and $\\mathrm{\\boldsymbol{F}}$. The MAP (so is the MLE) is unbiased, so that:\n",
    "\n",
    "$$\\langle \\hat{\\boldsymbol{\\theta}}_\\mathrm{MAP}\\rangle = {\\boldsymbol{\\theta}}_\\mathrm{true}\\,, $$\n",
    "\n",
    "with the true parameters, ${\\boldsymbol{\\theta}}_\\mathrm{true}$, which are often referred to as **fiducial parameters**. The average of the matrix $\\mathrm{\\boldsymbol{F}}$ takes a bit more work:\n",
    "\n",
    "$$\n",
    "\\langle\\mathrm{{F}_{\\alpha\\beta}}\\rangle = \\left\\langle\\frac{\\partial^2\\chi^2}{\\partial {\\theta}^\\alpha\\partial {\\theta}^\\beta}\\right\\rangle_{{\\boldsymbol{\\theta}}_\\mathrm{true}} = -\\left\\langle\\frac{\\partial^2\\ln L(\\boldsymbol{D}|\\boldsymbol{\\theta})}{\\partial {\\theta}^\\alpha\\partial {\\theta}^\\beta}\\right\\rangle_{{\\boldsymbol{\\theta}}_\\mathrm{true}} = -\\left\\langle \\frac{\\partial}{\\partial\\theta^\\alpha} \\frac{1}{L(\\boldsymbol{D}|\\boldsymbol{\\theta})}\\frac{\\partial L(\\boldsymbol{D}|\\boldsymbol{\\theta})}{\\partial {\\theta}^\\beta}\\right\\rangle_{{\\boldsymbol{\\theta}}_\\mathrm{true}} = -\\left\\langle \\frac{\\partial\\ln L(\\boldsymbol{D}|\\boldsymbol{\\theta})}{\\partial\\theta^\\alpha}\\frac{\\partial\\ln L(\\boldsymbol{D}|\\boldsymbol{\\theta})}{\\partial\\theta^\\beta}\\right\\rangle_{{\\boldsymbol{\\theta}}_\\mathrm{true}} \\,\n",
    "$$\n",
    "\n",
    "since $\\langle L\\rangle$ is a constant, so that the derivative vanishes. We will refer to $\\langle\\mathrm{{F}_{\\alpha\\beta}}\\rangle $ simply as $F_{\\alpha\\beta}$ and as the Fisher matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cramér-Rao bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shed a bit more light onto the definition and interpretation of the Fisher matrix. Let us consider an unbiased estimator $\\hat{\\boldsymbol\\theta}_0$ which estimates the parameters from some measurement $\\boldsymbol{x}$. The covariance of the estimator is\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathrm{C}} = \\langle (\\hat{\\boldsymbol{\\theta}} - {\\boldsymbol{\\theta}}_0)\\otimes(\\hat{\\boldsymbol{\\theta}} - {\\boldsymbol{\\theta}}_0)\\rangle\\,.\n",
    "$$\n",
    "\n",
    "Let us define vector, $b({\\boldsymbol{\\theta}}_0, {\\boldsymbol{x}})\\in\\mathbb{R}^{2n}$, which itself is a random variable:\n",
    "\n",
    "$$\n",
    "b({\\boldsymbol{\\theta}}_0, {\\boldsymbol{x}}) \\coloneqq\n",
    "\\begin{pmatrix}\n",
    "\\hat{\\boldsymbol{\\theta}} - {\\boldsymbol{\\theta}}_0\\\\\n",
    "\\nabla_\\theta\\ln L\n",
    "\\end{pmatrix} \\,.\n",
    "$$\n",
    "\n",
    "Out of this vector, we can now construct a positive definite matrix: \n",
    "\n",
    "$$\n",
    "0 \\preceq \\boldsymbol{\\mathrm{B}}\\coloneqq \\langle b({\\boldsymbol{\\theta}}_0, {\\boldsymbol{x}})\\otimes b({\\boldsymbol{\\theta}}_0, {\\boldsymbol{x}}) \\rangle = \n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{\\mathrm{C}} & \\boldsymbol{\\mathrm{U}} \\\\\n",
    "\\boldsymbol{\\mathrm{U}} ^\\mathrm{T} & \\boldsymbol{\\mathrm{F}} \n",
    "\\end{pmatrix}\\,,\n",
    "$$\n",
    "with the matrix $\\boldsymbol{\\mathrm{U}}$ defined as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathrm{U}}\\coloneqq \\langle  (\\hat{\\boldsymbol{\\theta}} - {\\boldsymbol{\\theta}}_0 )\\otimes \\nabla_\\theta\\ln L\\rangle\\,.\n",
    "$$\n",
    "\n",
    "Recall that $\\preceq$ and $\\succeq$ are relation operators which are in the case used here referred to the eigenvalues of a matrix.\n",
    "Since both $\\boldsymbol{\\mathrm{F}}$ and the block matrix defined above, $\\boldsymbol{\\mathrm{B}}$ are both positive semi-definite we know that the Schur complement of $\\boldsymbol{\\mathrm{F}}$ in $\\boldsymbol{\\mathrm{B}}$ must also be positive semi-definite. The latter is given by:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathrm{C}}  - \\boldsymbol{\\mathrm{U}} \\boldsymbol{\\mathrm{F}}^{-1} \\boldsymbol{\\mathrm{U}}^\\mathrm{T}\\;.\n",
    "$$\n",
    "\n",
    "Thus, we can conclude that $\\boldsymbol{\\mathrm{C}} \\succeq \\boldsymbol{\\mathrm{U}} \\boldsymbol{\\mathrm{F}}^{-1} \\boldsymbol{\\mathrm{U}}^\\mathrm{T}$. Let us now first note that the expectation value of $\\nabla_\\theta\\ln L$ vanishes. This is easy to see via\n",
    "\n",
    "$$\n",
    "\\langle \\nabla_\\theta\\ln L\n",
    " \\rangle =  \\int \\mathrm{d}\\boldsymbol{x}  \\frac{L(\\boldsymbol{x}|\\boldsymbol{\\theta})}{L(\\boldsymbol{x}|\\boldsymbol{\\theta})}  \\nabla_\\theta L(\\boldsymbol{x}|\\boldsymbol{\\theta})= 0\\,,\n",
    "$$\n",
    "\n",
    "hence we can write $\\boldsymbol{\\mathrm{U}}^\\mathrm{T} = \\langle \\hat{\\boldsymbol{\\theta}}^\\mathrm{T}\\nabla_\\theta \\ln L\\rangle$. We can furthermore pull out the differentiation from under the integral:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathrm{U}} = \\nabla_\\theta \\int\\mathrm{d}\\boldsymbol{x} \\, \\hat{\\boldsymbol{\\theta}}^\\mathrm{T} L(\\boldsymbol{x}|\\boldsymbol{\\theta}) =  \\nabla_\\theta \\boldsymbol{\\theta}^\\mathrm{T} = \\mathbb{1}\\,.\n",
    "$$\n",
    "\n",
    "The matrix $\\boldsymbol{\\mathrm{U}}$ is therefore the identity and we arrive at the following important conclusion for any unbiased estimator:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\boldsymbol{\\mathrm{C}} \\succeq \\boldsymbol{\\mathrm{F}}^{-1}\\,. }\n",
    "$$\n",
    "This is the Cramér-Rao inequality and it states that the variance of any unbiased estimator cannot be less than the variance derived from the Fisher matrix. Therefore, the Fisher matrix provides us with a lower bound on the statistical uncertainty achievable with a given experimental design. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fisher matrix for a Gaussian likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have not specified the functional form of the likelihood. Remember, the Fisher matrix assumes that the posterior is Gaussian, not the likelihood, which is in principle completely arbitrary. For a Gaussian likelihood, however, the Fisher matrix can be brought into a very nice form. So let us assume we have data $\\boldsymbol{x}$ with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\mathrm{C}}$, following an $d$-dimensional Gaussian distribution in data space. Recall that $\\langle\\boldsymbol{x}\\rangle =\\boldsymbol{\\mu}$ and $\\langle\\boldsymbol{\\mathrm{D}}\\rangle \\coloneqq \\langle(\\boldsymbol{x} - \\boldsymbol{\\mu})\\otimes (\\boldsymbol{x} - \\boldsymbol{\\mu})\\rangle = \\boldsymbol{\\mathrm{C}}$. Using $\\ln\\mathrm{det}\\boldsymbol{\\mathrm{C}} = \\mathrm{tr}\\ln\\boldsymbol{\\mathrm{C}}$, we can write the negative log-likelihood as\n",
    "\n",
    "$$\n",
    "-\\ln L = \\frac{1}{2}\\mathrm{tr}\\left(\\ln \\boldsymbol{\\mathrm{C}} + \\boldsymbol{\\mathrm{C}}^{-1}\\boldsymbol{\\mathrm{D}}\\right) + \\mathrm{const}\\,,\n",
    "$$\n",
    "\n",
    "Assuming that both the covariance and the mean depend on the model parameters $\\boldsymbol{\\theta}$ abd by using the expression for the Fisher matrix, it is straightforward to show (see problem sheet 8) that we can express it as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mathrm{F}}_{\\alpha\\beta} = \\frac{1}{2}\\mathrm{tr}\\left(\\boldsymbol{\\mathrm{C}}^{-1} \\frac{\\partial\\boldsymbol{\\mathrm{C}}}{\\partial\\boldsymbol{\\theta}^\\alpha}\\boldsymbol{\\mathrm{C}}^{-1} \\frac{\\partial\\boldsymbol{\\mathrm{C}}}{\\partial\\boldsymbol{\\theta}^\\beta} + 2\\boldsymbol{\\mathrm{C}}^{-1} \\frac{\\partial \\boldsymbol{\\mu}}{\\partial\\boldsymbol{\\theta}^\\alpha} \\frac{\\partial \\boldsymbol{\\mu}^\\mathrm{T}}{\\partial\\boldsymbol{\\theta}^\\beta} \\right)\\,. \n",
    "$$\n",
    "\n",
    "Since the likelihood is (assumed to be) Gaussian in many applications, this expression is incredibly useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the Fisher matrix\n",
    "\n",
    "In this section, we will take a look into the properties of the Fisher matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change of parameters\n",
    "Suppose we switch bijectively from an original set of parameters $\\boldsymbol{\\theta}^\\beta$ to a new set $\\tilde{\\boldsymbol{\\theta}}^\\alpha (\\boldsymbol{\\theta}^\\beta)$. \n",
    "$$\n",
    "\\boldsymbol{\\mathrm{F}}_{\\tilde\\theta} = \\boldsymbol{\\mathrm{J}}^\\mathrm{T}\\boldsymbol{\\mathrm{F}}_{\\theta}\\boldsymbol{\\mathrm{J}}\\,,\n",
    "$$\n",
    "where $\\boldsymbol{\\mathrm{J}}^\\alpha_\\beta = \\frac{\\partial \\theta^\\alpha}{\\partial\\tilde\\theta^\\beta}$, is the Jacobian of the coordinate transformation. A question is now, what happens to the volume element in the posterior due to the transformation, implying an additional term scaling logarithmically in the Jacobian. However, this term would immediately destroy the assumption of Gaussianity in the posterior. Near the best-fit point, one thus assumes that the volume element is a constant factor and is simply absorbed in the normalisation.\n",
    "\n",
    "### Marginalisation\n",
    "If we want to marginalise over a set of parameters $\\boldsymbol{\\Theta} \\subset \\{\\theta_i\\}$, we remove the corresponding rows and columns from the inverse of the original Fisher matrix $\\boldsymbol{\\mathrm{F}}$. This reduced inverse $\\boldsymbol{\\mathrm{F}}^{-1}_\\Theta$, with all rows and columns corresponding to $\\boldsymbol{\\Theta}$ removed, can itself be inverted to yield the Fisher matrix $\\boldsymbol{\\mathrm{F}}_\\Theta$ marginalised over all parameters $\\theta\\in\\boldsymbol{\\Theta}$.\n",
    "\n",
    "### Conditioning\n",
    "If, instead, we want to condition (maximise) the posterior on a set of parameters $\\boldsymbol{\\Theta} \\subset \\{\\theta_i\\}$, we simply remove the corresponding rows and columns from the of the original Fisher matrix $\\boldsymbol{\\mathrm{F}}$ to obtain $\\boldsymbol{\\mathrm{F}}_\\Theta$ conditioned on all parameters $\\theta\\in\\boldsymbol{\\Theta}$.\n",
    "\n",
    "### Adding Fisher matrices and priors\n",
    "A prior can be assigned a Fisher matrix, $\\boldsymbol{\\mathrm{F}}_\\mathrm{prior}$, i.e. it is assumed to be a multidimensional Gaussian. The total Fisher matrix of independent experiments (or prior information) is simply the sum of the individual Fisher matrices:\n",
    "\n",
    "$$\n",
    " \\boldsymbol{\\mathrm{F}}_\\mathrm{tot}  = \\boldsymbol{\\mathrm{F}}_\\mathrm{prior} + \\boldsymbol{\\mathrm{F}}_\\mathrm{experiment}\\,.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheat sheet for Fisher matrix\n",
    "\n",
    "- Transforms as a rank-2 tensor, so just apply the Jacobian twice (from the right and the left).\n",
    "- Conditioning: remove rows and columns of the corresponding parameters directly from the Fisher matrix.\n",
    "- Marginalisation: remove rows and columns of the corresponding parameters from the inverse Fisher matrix.\n",
    "- Combine: for independent experiments add the Fisher matrices. If one experiment only constraints a subset of parameters of the other, fill that Fisher matrices zeros for those parameters.\n",
    "- Confidence regions: the confidence intervals from a Fisher forecasts are given by ellipses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher matrix as information and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part, we want to make a connection to information theory and entropy. Assume a random process with probabilities $\\{p_i\\},\\, i=1,\\dots,N$. What is a good way to measure the information contained in it? Let us assume that our measure of information entropy, $S$ has to satisfy certain conditions:\n",
    "\n",
    "- If we know the the outcome with certainty, i.e. $\\forall p_i = 0,\\, i\\neq j$, $p_j = 1$, the uncertainty, or entropy, should vanish: $S=0$.\n",
    "- If all outcomes are equally probable, $S$ should be maximal.\n",
    "- $S\\geq 0$\n",
    "\n",
    "A good measure for this is the Shannon entropy:\n",
    "\n",
    "$$\n",
    "S = -\\sum_i p_i\\ln p_i = - \\int\\mathrm{d}x \\,p(x) \\ln p(x)\\,, \n",
    "$$\n",
    "\n",
    "which can easily be checked to fulfil all the above conditions. Let us look at the two limiting cases:\n",
    "\n",
    "1. $S=0$: this is the case if and only if all but one of all the $p_i$ vanishes. In this case, there is no entropy and no uncertainty\n",
    "\n",
    "2. $S = \\mathrm{maximal}$, this is the case if all probabilities are equal. In this situation we find $p_i = 1/\\Omega$, where $\\Omega$ is the number of possible outcomes. THe entropy is now: $$ S = \\frac{1}{\\Omega}\\sum_{i=1}^\\Omega \\ln\\Omega = \\ln \\Omega\\,. $$ The information entropy if all states are equally probable is therefore equal to the logarithm of all possible outcomes, or put differently of the number of microstates in a system.\n",
    "\n",
    "Looking closer at the definition of the Shannon entropy we recognise it to be the averaged log-likelihood. The Fisher matrix is hence a measure of the covariance of the change in information or entropy when changing the degrees of freedom of a statistical system. Furthermore, we again find the additivity of the Fisher matrix in the entropy measure, if the probabilities are independent:\n",
    "\n",
    "$$\n",
    "S = -\\sum_{i,j}p_{ij}\\ln p_{ij} = - \\sum_{i,j}p_i q_j (\\ln p_i + \\ln q_j) = - \\sum_i p_i \\ln p_i - \\sum_j q_j \\ln q_j\\,.\n",
    "$$\n",
    "\n",
    "Lastly, it should be noted that the Shannon entropy is not invariant under coordinate transformation for continuous variables, since there is an additional Jacobian in the logarithm. This is often remedied by the introduction of relative entropies. The most famous example is possibly the Kullback-Leibler divergence:\n",
    "\n",
    "$$\n",
    "\\Delta S = -\\int \\mathrm{d}x\\, p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right)\\,,\n",
    "$$\n",
    "but there are other divergencies. Every divergence can locally be expanded into a quadratic form with the Fisher matrix as the corresponding operator.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levinpower_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
